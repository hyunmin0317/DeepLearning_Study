# chap 08-3 합성곱 신경망의 구조를 알아봅니다

2021.03.31

`합성곱 신경망에서 자주 사용하는 활성화 함수인 렐루(ReLU) 함수에 대해 알아보겠습니다.`

<br>

### 01. 렐루 함수에 대해 알아봅니다

* 이전까지의 활성화 함수
  * 은닝층: 시그모이드 함수
  * 출력층: 시그모이드 함수(이진 분류), 소프트맥스 함수(다중 분류)
* 렐루 함수
  * 주로 합성곱층에 적용되는 활성화 함수로, 합성곱 신경망의 성능을 더 높여줌
  * y = x (x>0), y = 0 (x<=0)

<br>

### 02. 렐루 함수 구현하기

* 두 매개변수 값 중 큰 값을 골라 반환하는 maximum() 함수를 통해 relu() 함수 구현

  ```python
  def relu(x):
      return np.maximum(x, 0)
      
  x = np.array([-1, 2, -3, 4, -5])
  relu(x)	# array([0, 2, 0, 4, 0])
  ```

* 텐서플로가 제공하는 렐루 함수 relu() - Tensor 객체를 반환하므로 화면에 출력하려면 넘파이로 변환해야 함

  ```python
  r_out = tf.nn.relu(x)
  r_out.numpy()	# array([0, 2, 0, 4, 0])
  ```

<br>

### 03. 렐루 함수의 도함수를 알아봅니다

* 렐루 함수의 도함수는 입력이 0보다 크면 1이고 입력이 0보다 작으면 0
* y = 1 (x>0), y = 0 (x<=0)

<br>

### 04. 합성곱 신경망에서 일어나는 일들과 구조를 알아봅니다

* 합성곱 신경망에 주입될 입력 데이터에는 채널이 있습니다

  * 합성곱 신경망은 이미지의 2차원 형태를 입력으로 그대로 사용하므로 이미지 정보가 손상되지 않는다는 장점이 있음
  * 채널(channel): 이미지의 픽셀이 가진 색상을 표현하기 위해 필요한 정보로 이미지는 채널이라는 차원을 하나 더 가짐

* 합성곱층에서 일어나는 일을 알아봅니다

  * 이미지의 모든 채널에 합성곱이 한 번에 적용되어야 하므로 커널의 마지막 차원은 입력 채널의 개수와 동일해야 함

  * 커널의 크기는 보통 3 * 3 또는 5 * 5로 커널 배열의 크기가 다를 때 마지막 차원의 개수를 동일하게 맞추어야 함

  * 합성곱의 전체 과정

    <img src="https://github.com/hyunmin0317/DeepLearning_Study/blob/master/chap08/section3/image01.PNG?raw=true" alt="image01.PNG" style="zoom:67%;" />

    * 합성곱을 4번 수행하므로 2 * 2 크기의 특성 맵이 만들어지고 입력 채널은 커널의 채널과 각각 합성곱을 수행함

* 풀링층에서 일어나는 일을 알아봅니다

  <img src="https://github.com/hyunmin0317/DeepLearning_Study/blob/master/chap08/section3/image02.PNG?raw=true" alt="image02.PNG" style="zoom:67%;" />

  * 합성곱층을 통해 만들어진 특성 맵에 활성화 함수로 렐루 함수를 적용하고 풀링을 적용
  * 풀링은 특성 맵의 크기를 줄여주고 이때 채널의 크기는 줄어들지 않음

<br>

### 05. 특성 맵을 펼쳐 완전 연결 신경망에 주입합니다

<img src="https://github.com/hyunmin0317/DeepLearning_Study/blob/master/chap08/section3/image03.PNG?raw=true" alt="image03.PNG" style="zoom:67%;" />

* 합성곱층과 풀링층을 통과시켜 얻은 특성 맵은 일렬로 펼쳐 완전 연결층에 입력으로 주임됨
* 완전 연결층은 한 신경망 안에 여러 개가 들어 있을 수도 있으며 완전 연결층의 출력은 출력층의 뉴런과 연결됨